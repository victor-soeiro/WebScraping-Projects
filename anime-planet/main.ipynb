{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d551b913-a669-409c-bffb-06432b57656d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Anime-Planet: Web Scraping Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a83b3-75ec-4cbe-83dd-7adc32e3f23b",
   "metadata": {},
   "source": [
    "<img src=\"cover.jpg\" style=\"margin-top: 20px; margin-bottom: 20px; width: 800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914f98f-468e-44e4-8662-691c75a1bbe2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Preludes** \n",
    "\n",
    "Comics are popular among people of all ages. It's a form of art, a narrative expressed as draws. Since the late 20th century, the publishers have translated mangas (comics originating from Japan) to the world. The fast growth of the popularity gained a significant worldwide audience. In 2021 in the US, mangas made up [76% of overall comics and graphic novels sales](https://www.animenewsnetwork.com/news/2022-03-01/npd-bookscan-via-the-beat-manga-made-up-76.71-percent-of-adult-fiction-graphic-novel-sales-in-2021/.182296) according to NPD Group. This projects aims to scrape the mangas available on [Anime-Planet](https://www.anime-planet.com/), a great source to search for mangas and animes. This data will be stored on a CSV file, and available on GitHub and Kaggle for study purposes.\n",
    "\n",
    "I will consider that mangas includes manhwa (originating from Korea) and manhua (originating  from China).\n",
    "\n",
    "**Disclaimer:** This is a personal project to practice webscraping skills and exploratory data analysis. I do not recommend to use for other purposes. Use it at your own risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00746e7-25b4-4c6b-b9db-133854197118",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Yoshi Yoshi**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced21c2b-1a1e-4957-83b1-082538d8aa20",
   "metadata": {},
   "source": [
    "To those who didn't understood the term \"Yoshi Yoshi\", it means to start an activity with entusiasm. Let's start the WebScraping on Anime-Planet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d6508-55ef-40d1-9398-cdd8dd9d0afe",
   "metadata": {},
   "source": [
    "### **Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99919471-c66d-42ba-b9e2-b32f20c60f05",
   "metadata": {},
   "source": [
    "For most web scraping projects, I will use only requests and bs4. In this project, I will use pandas to facilitate the file handling and NumPy to handle the NaN values (rarely it will occur). I'm using an Anaconda environment, so it's easy to import the libraries, but if you want to replicate, maybe you need to install the libraries with the pip command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e8d8b30-545f-4b06-97ae-679381e869d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tnrange\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e11334e-b9d9-4999-a685-a5b32adb1dcf",
   "metadata": {},
   "source": [
    "### **Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f30a6-0541-4104-90d4-b1a43a02ca1a",
   "metadata": {},
   "source": [
    "All mangas are available on a path on Anime-Planet Website. The page system uses an URL parameter to access the content. Let's define the URL and the end page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e06cf4-507f-4da3-b041-e02ded2b70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://www.anime-planet.com/manga/all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9218fa0-816f-4c36-be1b-4214637200ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_page = 2028"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24aa12a-704b-4505-a0b8-3dc801e27ff9",
   "metadata": {},
   "source": [
    "### **Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64b84da-9fdd-4867-8712-7227ccecb3eb",
   "metadata": {},
   "source": [
    "I will encapsulate the repetitive code on a function to modify in case of need. \n",
    "\n",
    "One of the errors of web scraping is to call an object function that is None. It will raise an error that will interrupt the scraping process. I already checked the Anime-Planet website, and we only need to create a function to check if the object is None, if True returns the text on it or if False returns a NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467aa123-47a4-4d38-b5e5-ba1b1e96fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_text(value):\n",
    "    if value:\n",
    "        return value.text\n",
    "    \n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b458ba-9f3e-4ecc-83ff-573ccb9eb60b",
   "metadata": {},
   "source": [
    "It's funny how the data is available on the page. When the mouse it's over the manga, it will display a window containing all the metadata. As we are scraping, all the hover information is contained in the title tag of the item HTML. This string needs to be parsed again on another BeautifulSoup object. \n",
    "\n",
    "Now, it's easy to scrape all the manga metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c29c0bd0-afce-4f00-8031-2da5eab47c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_scraper(item):\n",
    "    info = item.a['title']\n",
    "    info_bs = BeautifulSoup(info, 'html.parser')\n",
    "    \n",
    "    title = check_text(info_bs.h5)\n",
    "    description = check_text(info_bs.p)\n",
    "    rating = check_text(info_bs.find('div', attrs={'class': 'ttRating'}))\n",
    "    year = check_text(info_bs.find('li', attrs={'class': 'iconYear'}))\n",
    "    if year:\n",
    "        year = str(year).split(' - ')[0]\n",
    "    \n",
    "    if info_bs.h4:\n",
    "        tags = [t.text for t in info_bs.h4.nextSibling.findAll('li')]\n",
    "    else:\n",
    "        tags = []\n",
    "    \n",
    "    cover = item.a.div.img['data-src']\n",
    "    \n",
    "    return [title, description, rating, year, tags, cover]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97153f-39f7-487f-8d0b-66af39d0744c",
   "metadata": {},
   "source": [
    "As we are dealing with over 2 thousand pages, if one error occurs in the process, we will lose all the progress. One of the possibilities is to write directly into a file, but it will be a heavy memory consumer. As we are dealing with a notebook, I will write the data on a dictionary, as the key is the page, and the value it's the manga metadata.\n",
    "\n",
    "Maybe it's not the best idea, but it will be enough for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dbd67b0-957d-4ef7-978b-d3856c21b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9391fc70-bcef-4a31-9598-0790c071a75f",
   "metadata": {},
   "source": [
    "And for the main part of the project, let's scrape the pages with a recursive function. We can do it in multiple ways. For visualization with tqdm, we will loop a range object containing an array of pages to call the function to obtain the metadata. But first, let's define the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42c8ba22-c3e7-42e4-889f-e58cd648d749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraper(page=1):   \n",
    "    req = requests.get(main_url, params={'page': page})\n",
    "    if req.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    bs = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    container = bs.find('ul', attrs={'class': 'cardDeck'})\n",
    "    items = container.findAll('li')\n",
    "\n",
    "    data = [item_scraper(i) for i in items]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2ae4b-1855-4078-b317-b2f364598ca3",
   "metadata": {},
   "source": [
    "Now we can loop the pages with the function using tqdm to observe the loop progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82591fe-99a3-4b7d-a30d-997896235bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c829a6f6270d40d8b9918f6b0fbad8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pages:   0%|          | 0/2028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tnrange(end_page, desc='Pages'):\n",
    "    page = i + 1\n",
    "    \n",
    "    if pages_data.get(page, []):\n",
    "        continue\n",
    "        \n",
    "    data = scraper(page+1)\n",
    "    pages_data[page] = data\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4318c4f-0e46-4804-8917-13dadfccf767",
   "metadata": {},
   "source": [
    "The data are partitioned in the dictionary. Let's unite all the data and save it into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b4d914-6878-4cb0-8f8d-02a70d7e3c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['title', 'description', 'rating', 'year', 'tags', 'cover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f0b31bb-b037-44d2-82c9-01abf145c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = []\n",
    "for i in pages_data.values():\n",
    "    full_data.extend(i)\n",
    "    \n",
    "pd.DataFrame(full_data, columns=headers).to_csv('data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
