{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb7a779-b1a0-494b-ac83-dcbeedeac4a5",
   "metadata": {},
   "source": [
    "## **Webtoons - WebScraping Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5963968-1152-49b2-bf18-30c0025c3e0c",
   "metadata": {},
   "source": [
    "<img src=\"https://media-exp1.licdn.com/dms/image/C561BAQF7mhzUFFpGQg/company-background_10000/0/1588199218989?e=2147483647&v=beta&t=jmdPaw4ITMyXHwfx5vv7QJ6TnzTNhYhpuX3wUfLrCgg\" width=\"100%\">\n",
    "\n",
    "> **Disclaimer:** This is a personal project to practice webscraping skills and exploratory data analysis. I do not recommend to use for other purposes. Use it at your own risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14603b-37f3-45ec-b4e7-e3aaeb19406d",
   "metadata": {},
   "source": [
    "### **Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f86eaab-f643-4015-8866-424dc0c0aa64",
   "metadata": {},
   "source": [
    "All comics on webtoons are on only one page, but it has extra information on their unique page. The scraping process will use only the main tools. \n",
    "\n",
    "> **If you wanna replicate, maybe you need to install some of the packages with PIP command.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73a4744-3921-4110-bcdf-793cc499d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b09bd5-eafe-4394-b078-e32b0481c96a",
   "metadata": {},
   "source": [
    "Let's define the URL to request the comics HTML page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e812c94-d5b2-408d-8244-9ae901ad61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.webtoons.com/en/genre'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a22c06-ac8d-453d-860c-37364899af59",
   "metadata": {},
   "source": [
    "Let's start the extracting process of the comics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e2e4fa-8eca-4772-808a-33f376b12e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req = requests.get(url)\n",
    "req.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa196a1-988c-49b0-85f2-716bceba0824",
   "metadata": {},
   "source": [
    "Great! Parsing the HTML to a BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b7f582-3857-4a94-8666-5542bbbc0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f6d63-22d4-49ab-b6ee-cebc13b47fd0",
   "metadata": {},
   "source": [
    "Each container has comics from a genre. It's a total of 16 containers and over 1400 comics. Let's collect all the comics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d372824-d224-4a88-a075-433b10a2cc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containers = soup.findAll('ul', attrs={'class': 'card_lst'})\n",
    "len(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a26b79a1-80de-4e14-af51-df853367579e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [i for c in containers for i in c.findAll('li')]\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab276a3d-99ff-4d85-9bb1-a49e87bd0ffc",
   "metadata": {},
   "source": [
    "To collect the release date information requires knowing how many episodes pages have to make another request. Let's encapsulate it on a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfbef46-176e-491c-b09a-226028a69669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_released_information(url, page):\n",
    "    req = requests.get(url, params={'page': page})\n",
    "    if req.status_code != 200:\n",
    "        raise requests.ConnectionError(f'failed to connect to {url}. [{req.status_code}]')\n",
    "    \n",
    "    bs = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    first_episode = bs.find('ul', attrs={'id': '_listUl'}).findAll('li')[-1]\n",
    "    released_data = first_episode.find('span', attrs={'class': 'date'}).text.strip()\n",
    "    \n",
    "    return released_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7287d22-ae4a-48c9-b708-9ca5551235ec",
   "metadata": {},
   "source": [
    "The comic information is on a unique page. Let's encapsulate it too on a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5295f7-7676-401d-808e-febbfae73916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_information(url):\n",
    "    req = requests.get(url)\n",
    "    if req.status_code != 200:\n",
    "        raise requests.ConnectionError(f'failed to connect to {url}. [{req.status_code}]')\n",
    "    \n",
    "    bs = BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    item_data = {}\n",
    "    \n",
    "    item_data['webtoon_id'] = url.split('=')[-1]\n",
    "    item_data['title'] = bs.find('h1', attrs={'class': 'subj'}).text.strip() \n",
    "    item_data['genre'] = bs.find('h2', attrs={'class': 'genre'}).text.strip()\n",
    "    item_data['thumbnail'] = bs.find('span', attrs={'class': 'thmb'}).img['src']\n",
    "    item_data['summary'] = bs.find('p', attrs={'class': 'summary'}).text.strip()\n",
    "    item_data['episodes'] = bs.find('ul', attrs={'id': '_listUl'}).findAll('li')[0]['data-episode-no']\n",
    "    \n",
    "    authors_container = bs.find('div', attrs={'class': '_authorInnerContent'})\n",
    "    authors_categories = authors_container.findAll('p', attrs={'class': 'by'})\n",
    "    authors_names = authors_container.findAll('h3', attrs={'class': 'title'})\n",
    "    for c, n in zip(authors_categories, authors_names):\n",
    "        item_data[c.text.strip()] = n.text.strip()\n",
    "        \n",
    "    stats_container = bs.find('p', attrs={'class': 'summary'}).parent.find('ul').findAll('li')\n",
    "    for s in stats_container:\n",
    "        item_data[s.span.text.strip()] = s.em.text.strip()\n",
    "    \n",
    "    last_page = int(math.ceil(int(item_data['episodes']) / 10))\n",
    "    item_data['released_date'] = get_item_released_information(url, last_page)\n",
    "    item_data['url'] = url\n",
    "    \n",
    "    return item_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd10fc-1d2c-4502-b2dd-588f026c64fe",
   "metadata": {},
   "source": [
    "The collected data will be on a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2e2e632-fc55-46f0-b68d-b656e15ddfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b06cf-a1da-4692-b2c7-a2272304395f",
   "metadata": {},
   "source": [
    "Now, let's start the looping process to collect all the information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb45ce00-048f-4878-9a96-cfa6d3766a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae67d708-3887-4d20-8163-a4c539644ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804bca690f404893bc78bf747dff1abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(items):    \n",
    "    href = i.a['href']\n",
    "    if href in done:\n",
    "        continue\n",
    "        \n",
    "    item_data = get_item_information(href)\n",
    "    item_data['cover'] = i.find('img')['src']\n",
    "    item_data['likes'] = i.find('em', attrs={'class': 'grade_num'}).text.strip()  \n",
    "    \n",
    "    data.append(item_data)\n",
    "    done.append(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6c6482-3bf5-4729-9d36-b2a8db9325c3",
   "metadata": {},
   "source": [
    "The scraping process took around 35 minutes, with two requests per comic. \n",
    "\n",
    "As the data are a list of dictionaries, we don't need to define all the column's names. Let's parse the data to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcc4cbf1-5cda-4ed5-af16-0827f1c7b544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>webtoon_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>summary</th>\n",
       "      <th>episodes</th>\n",
       "      <th>Created by</th>\n",
       "      <th>view</th>\n",
       "      <th>subscribe</th>\n",
       "      <th>grade</th>\n",
       "      <th>released_date</th>\n",
       "      <th>url</th>\n",
       "      <th>cover</th>\n",
       "      <th>likes</th>\n",
       "      <th>Written by</th>\n",
       "      <th>Art by</th>\n",
       "      <th>Adapted by</th>\n",
       "      <th>Original work by</th>\n",
       "      <th>Assisted by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1218</td>\n",
       "      <td>Let's Play</td>\n",
       "      <td>Romance</td>\n",
       "      <td>https://webtoon-phinf.pstatic.net/20210629_103...</td>\n",
       "      <td>She’s young, single and about to achieve her d...</td>\n",
       "      <td>171</td>\n",
       "      <td>Leeanne M. Krecic (Mongie)</td>\n",
       "      <td>606.5M</td>\n",
       "      <td>4.6M</td>\n",
       "      <td>9.59</td>\n",
       "      <td>Nov 6, 2017</td>\n",
       "      <td>https://www.webtoons.com/en/romance/letsplay/l...</td>\n",
       "      <td>https://webtoon-phinf.pstatic.net/20210629_163...</td>\n",
       "      <td>37.2M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1436</td>\n",
       "      <td>True Beauty</td>\n",
       "      <td>Romance</td>\n",
       "      <td>https://webtoon-phinf.pstatic.net/20210129_175...</td>\n",
       "      <td>After binge-watching beauty videos online, a s...</td>\n",
       "      <td>197</td>\n",
       "      <td>Yaongyi</td>\n",
       "      <td>874M</td>\n",
       "      <td>7M</td>\n",
       "      <td>9.53</td>\n",
       "      <td>Aug 15, 2018</td>\n",
       "      <td>https://www.webtoons.com/en/romance/truebeauty...</td>\n",
       "      <td>https://webtoon-phinf.pstatic.net/20210129_65/...</td>\n",
       "      <td>46.2M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2135</td>\n",
       "      <td>The Remarried Empress</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>https://webtoon-phinf.pstatic.net/20200904_29/...</td>\n",
       "      <td>Navier Ellie Trovi was an empress perfect in e...</td>\n",
       "      <td>110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231.3M</td>\n",
       "      <td>2.5M</td>\n",
       "      <td>9.87</td>\n",
       "      <td>Sep 5, 2020</td>\n",
       "      <td>https://www.webtoons.com/en/fantasy/the-remarr...</td>\n",
       "      <td>https://webtoon-phinf.pstatic.net/20200904_268...</td>\n",
       "      <td>21.2M</td>\n",
       "      <td>Alphatart</td>\n",
       "      <td>Sumpul</td>\n",
       "      <td>HereLee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1798</td>\n",
       "      <td>Midnight Poppy Land</td>\n",
       "      <td>Romance</td>\n",
       "      <td>https://webtoon-phinf.pstatic.net/20191119_132...</td>\n",
       "      <td>After making a grisly discovery in the country...</td>\n",
       "      <td>99</td>\n",
       "      <td>Lilydusk</td>\n",
       "      <td>198.8M</td>\n",
       "      <td>2.3M</td>\n",
       "      <td>9.80</td>\n",
       "      <td>Nov 22, 2019</td>\n",
       "      <td>https://www.webtoons.com/en/romance/midnight-p...</td>\n",
       "      <td>https://webtoon-phinf.pstatic.net/20191119_163...</td>\n",
       "      <td>13.5M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3416</td>\n",
       "      <td>Reunion</td>\n",
       "      <td>Romance</td>\n",
       "      <td>https://webtoon-phinf.pstatic.net/20220311_196...</td>\n",
       "      <td>After moving away for a decade, Rhea returns t...</td>\n",
       "      <td>9</td>\n",
       "      <td>stephattyy</td>\n",
       "      <td>7.1M</td>\n",
       "      <td>629,872</td>\n",
       "      <td>9.77</td>\n",
       "      <td>Mar 17, 2022</td>\n",
       "      <td>https://www.webtoons.com/en/romance/reunion/li...</td>\n",
       "      <td>https://webtoon-phinf.pstatic.net/20220311_14/...</td>\n",
       "      <td>570,151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  webtoon_id                  title    genre  \\\n",
       "0       1218             Let's Play  Romance   \n",
       "1       1436            True Beauty  Romance   \n",
       "2       2135  The Remarried Empress  Fantasy   \n",
       "3       1798    Midnight Poppy Land  Romance   \n",
       "4       3416                Reunion  Romance   \n",
       "\n",
       "                                           thumbnail  \\\n",
       "0  https://webtoon-phinf.pstatic.net/20210629_103...   \n",
       "1  https://webtoon-phinf.pstatic.net/20210129_175...   \n",
       "2  https://webtoon-phinf.pstatic.net/20200904_29/...   \n",
       "3  https://webtoon-phinf.pstatic.net/20191119_132...   \n",
       "4  https://webtoon-phinf.pstatic.net/20220311_196...   \n",
       "\n",
       "                                             summary episodes  \\\n",
       "0  She’s young, single and about to achieve her d...      171   \n",
       "1  After binge-watching beauty videos online, a s...      197   \n",
       "2  Navier Ellie Trovi was an empress perfect in e...      110   \n",
       "3  After making a grisly discovery in the country...       99   \n",
       "4  After moving away for a decade, Rhea returns t...        9   \n",
       "\n",
       "                   Created by    view subscribe grade released_date  \\\n",
       "0  Leeanne M. Krecic (Mongie)  606.5M      4.6M  9.59   Nov 6, 2017   \n",
       "1                     Yaongyi    874M        7M  9.53  Aug 15, 2018   \n",
       "2                         NaN  231.3M      2.5M  9.87   Sep 5, 2020   \n",
       "3                    Lilydusk  198.8M      2.3M  9.80  Nov 22, 2019   \n",
       "4                  stephattyy    7.1M   629,872  9.77  Mar 17, 2022   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.webtoons.com/en/romance/letsplay/l...   \n",
       "1  https://www.webtoons.com/en/romance/truebeauty...   \n",
       "2  https://www.webtoons.com/en/fantasy/the-remarr...   \n",
       "3  https://www.webtoons.com/en/romance/midnight-p...   \n",
       "4  https://www.webtoons.com/en/romance/reunion/li...   \n",
       "\n",
       "                                               cover    likes Written by  \\\n",
       "0  https://webtoon-phinf.pstatic.net/20210629_163...    37.2M        NaN   \n",
       "1  https://webtoon-phinf.pstatic.net/20210129_65/...    46.2M        NaN   \n",
       "2  https://webtoon-phinf.pstatic.net/20200904_268...    21.2M  Alphatart   \n",
       "3  https://webtoon-phinf.pstatic.net/20191119_163...    13.5M        NaN   \n",
       "4  https://webtoon-phinf.pstatic.net/20220311_14/...  570,151        NaN   \n",
       "\n",
       "   Art by Adapted by Original work by Assisted by  \n",
       "0     NaN        NaN              NaN         NaN  \n",
       "1     NaN        NaN              NaN         NaN  \n",
       "2  Sumpul    HereLee              NaN         NaN  \n",
       "3     NaN        NaN              NaN         NaN  \n",
       "4     NaN        NaN              NaN         NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ca66d-5026-4372-9b61-a9d80aa54a57",
   "metadata": {},
   "source": [
    "And finally, let's save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41d9e1ea-113d-4e75-a87d-b05b1fd2e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ce0e1-5d8c-4d21-898a-66a04d70de44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
